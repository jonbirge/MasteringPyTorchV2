{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==1.9\n",
      "  Using cached torch-1.9.0-cp39-none-macosx_11_0_arm64.whl (41.5 MB)\n",
      "Requirement already satisfied: typing-extensions in /Users/Ashish.Jha/anaconda3/envs/python39/lib/python3.9/site-packages (from torch==1.9) (4.9.0)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.7.0 has a non-standard dependency specifier torch>=1.9.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.12.1\n",
      "    Uninstalling torch-1.12.1:\n",
      "      Successfully uninstalled torch-1.12.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchdata 0.4.1 requires torch==1.12.1, but you have torch 1.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-1.9.0\n",
      "Requirement already satisfied: torchtext==0.10 in /Users/Ashish.Jha/anaconda3/envs/python39/lib/python3.9/site-packages (0.10.0)\n",
      "Requirement already satisfied: tqdm in /Users/Ashish.Jha/anaconda3/envs/python39/lib/python3.9/site-packages (from torchtext==0.10) (4.66.2)\n",
      "Requirement already satisfied: requests in /Users/Ashish.Jha/anaconda3/envs/python39/lib/python3.9/site-packages (from torchtext==0.10) (2.31.0)\n",
      "Requirement already satisfied: torch in /Users/Ashish.Jha/anaconda3/envs/python39/lib/python3.9/site-packages (from torchtext==0.10) (1.9.0)\n",
      "Requirement already satisfied: numpy in /Users/Ashish.Jha/anaconda3/envs/python39/lib/python3.9/site-packages (from torchtext==0.10) (1.26.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/Ashish.Jha/anaconda3/envs/python39/lib/python3.9/site-packages (from requests->torchtext==0.10) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/Ashish.Jha/anaconda3/envs/python39/lib/python3.9/site-packages (from requests->torchtext==0.10) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/Ashish.Jha/anaconda3/envs/python39/lib/python3.9/site-packages (from requests->torchtext==0.10) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/Ashish.Jha/anaconda3/envs/python39/lib/python3.9/site-packages (from requests->torchtext==0.10) (2024.2.2)\n",
      "Requirement already satisfied: typing-extensions in /Users/Ashish.Jha/anaconda3/envs/python39/lib/python3.9/site-packages (from torch->torchtext==0.10) (4.9.0)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.7.0 has a non-standard dependency specifier torch>=1.9.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting matplotlib==3.8.3\n",
      "  Using cached matplotlib-3.8.3-cp39-cp39-macosx_11_0_arm64.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/Ashish.Jha/anaconda3/envs/python39/lib/python3.9/site-packages (from matplotlib==3.8.3) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/Ashish.Jha/anaconda3/envs/python39/lib/python3.9/site-packages (from matplotlib==3.8.3) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/Ashish.Jha/anaconda3/envs/python39/lib/python3.9/site-packages (from matplotlib==3.8.3) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/Ashish.Jha/anaconda3/envs/python39/lib/python3.9/site-packages (from matplotlib==3.8.3) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /Users/Ashish.Jha/anaconda3/envs/python39/lib/python3.9/site-packages (from matplotlib==3.8.3) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/Ashish.Jha/anaconda3/envs/python39/lib/python3.9/site-packages (from matplotlib==3.8.3) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /Users/Ashish.Jha/anaconda3/envs/python39/lib/python3.9/site-packages (from matplotlib==3.8.3) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/Ashish.Jha/anaconda3/envs/python39/lib/python3.9/site-packages (from matplotlib==3.8.3) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/Ashish.Jha/anaconda3/envs/python39/lib/python3.9/site-packages (from matplotlib==3.8.3) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/Ashish.Jha/anaconda3/envs/python39/lib/python3.9/site-packages (from matplotlib==3.8.3) (6.1.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/Ashish.Jha/anaconda3/envs/python39/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib==3.8.3) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/Ashish.Jha/anaconda3/envs/python39/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib==3.8.3) (1.16.0)\n",
      "Using cached matplotlib-3.8.3-cp39-cp39-macosx_11_0_arm64.whl (7.5 MB)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.7.0 has a non-standard dependency specifier torch>=1.9.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: matplotlib\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.5.2\n",
      "    Uninstalling matplotlib-3.5.2:\n",
      "      Successfully uninstalled matplotlib-3.5.2\n",
      "Successfully installed matplotlib-3.8.3\n"
     ]
    }
   ],
   "source": [
    "# requires python3.9\n",
    "!pip install torch==1.9\n",
    "!pip install torchtext==0.10\n",
    "!pip install matplotlib==3.8.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torchtext.legacy import datasets\n",
    "from torchtext.legacy import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_FIELD = data.Field(tokenize = data.get_tokenizer(\"basic_english\"), include_lengths = True)\n",
    "LABEL_FIELD = data.LabelField(dtype = torch.float)\n",
    "\n",
    "train_dataset, test_dataset = datasets.IMDB.splits(TEXT_FIELD, LABEL_FIELD)\n",
    "train_dataset, valid_dataset = train_dataset.split(random_state = random.seed(123))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCABULARY_SIZE = 25000\n",
    "\n",
    "TEXT_FIELD.build_vocab(train_dataset, \n",
    "                 max_size = MAX_VOCABULARY_SIZE)\n",
    "\n",
    "LABEL_FIELD.build_vocab(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_data_iterator, valid_data_iterator, test_data_iterator = data.BucketIterator.splits(\n",
    "    (train_dataset, valid_dataset, test_dataset), \n",
    "    batch_size = B_SIZE,\n",
    "    sort_within_batch = True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you are training using GPUs, we need to use the following function for the pack_padded_sequence method to work \n",
    "## (reference : https://discuss.pytorch.org/t/error-with-lengths-in-pack-padded-sequence/35517/3)\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, PackedSequence\n",
    "\n",
    "def cuda_pack_padded_sequence(input, lengths, batch_first=False, enforce_sorted=True):\n",
    "    lengths = torch.as_tensor(lengths, dtype=torch.int64)\n",
    "    lengths = lengths.cpu()\n",
    "    if enforce_sorted:\n",
    "        sorted_indices = None\n",
    "    else:\n",
    "        lengths, sorted_indices = torch.sort(lengths, descending=True)\n",
    "        sorted_indices = sorted_indices.to(input.device)\n",
    "    batch_dim = 0 if batch_first else 1\n",
    "    input = input.index_select(batch_dim, sorted_indices)\n",
    "\n",
    "    data, batch_sizes = \\\n",
    "    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)\n",
    "    return PackedSequence(data, batch_sizes, sorted_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocabulary_size, embedding_dimension, hidden_dimension, output_dimension, dropout, pad_index):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = nn.Embedding(vocabulary_size, embedding_dimension, padding_idx = pad_index)\n",
    "        self.lstm_layer = nn.LSTM(embedding_dimension, \n",
    "                           hidden_dimension, \n",
    "                           num_layers=1, \n",
    "                           bidirectional=True, \n",
    "                           dropout=dropout)\n",
    "        self.fc_layer = nn.Linear(hidden_dimension * 2, output_dimension)\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, sequence, sequence_lengths=None):\n",
    "        if sequence_lengths is None:\n",
    "            sequence_lengths = torch.LongTensor([len(sequence)])\n",
    "        \n",
    "        # sequence := (sequence_length, batch_size)\n",
    "        embedded_output = self.dropout_layer(self.embedding_layer(sequence))\n",
    "        \n",
    "        \n",
    "        # embedded_output := (sequence_length, batch_size, embedding_dimension)\n",
    "        if torch.cuda.is_available():\n",
    "            packed_embedded_output = cuda_pack_padded_sequence(embedded_output, sequence_lengths)\n",
    "        else:\n",
    "            packed_embedded_output = nn.utils.rnn.pack_padded_sequence(embedded_output, sequence_lengths)\n",
    "        \n",
    "        packed_output, (hidden_state, cell_state) = self.lstm_layer(packed_embedded_output)\n",
    "        # hidden_state := (num_layers * num_directions, batch_size, hidden_dimension)\n",
    "        # cell_state := (num_layers * num_directions, batch_size, hidden_dimension)\n",
    "        \n",
    "        op, op_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "        # op := (sequence_length, batch_size, hidden_dimension * num_directions)\n",
    "        \n",
    "        hidden_output = torch.cat((hidden_state[-2,:,:], hidden_state[-1,:,:]), dim = 1)        \n",
    "        # hidden_output := (batch_size, hidden_dimension * num_directions)\n",
    "        \n",
    "        return self.fc_layer(hidden_output)\n",
    "\n",
    "    \n",
    "INPUT_DIMENSION = len(TEXT_FIELD.vocab)\n",
    "EMBEDDING_DIMENSION = 100\n",
    "HIDDEN_DIMENSION = 32\n",
    "OUTPUT_DIMENSION = 1\n",
    "DROPOUT = 0.5\n",
    "PAD_INDEX = TEXT_FIELD.vocab.stoi[TEXT_FIELD.pad_token]\n",
    "\n",
    "lstm_model = LSTM(INPUT_DIMENSION, \n",
    "            EMBEDDING_DIMENSION, \n",
    "            HIDDEN_DIMENSION, \n",
    "            OUTPUT_DIMENSION, \n",
    "            DROPOUT, \n",
    "            PAD_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_INDEX = TEXT_FIELD.vocab.stoi[TEXT_FIELD.unk_token]\n",
    "\n",
    "lstm_model.embedding_layer.weight.data[UNK_INDEX] = torch.zeros(EMBEDDING_DIMENSION)\n",
    "lstm_model.embedding_layer.weight.data[PAD_INDEX] = torch.zeros(EMBEDDING_DIMENSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(lstm_model.parameters())\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "\n",
    "lstm_model = lstm_model.to(device)\n",
    "loss_func = loss_func.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_metric(predictions, ground_truth):\n",
    "    \"\"\"\n",
    "    Returns 0-1 accuracy for the given set of predictions and ground truth\n",
    "    \"\"\"\n",
    "    # round predictions to either 0 or 1\n",
    "    rounded_predictions = torch.round(torch.sigmoid(predictions))\n",
    "    success = (rounded_predictions == ground_truth).float() #convert into float for division \n",
    "    accuracy = success.sum() / len(success)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_iterator, optim, loss_func):\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "    model.train()\n",
    "    \n",
    "    for curr_batch in data_iterator:\n",
    "        optim.zero_grad()\n",
    "        sequence, sequence_lengths = curr_batch.text\n",
    "        preds = lstm_model(sequence, sequence_lengths)\n",
    "        \n",
    "        loss_curr = loss_func(preds, curr_batch.label)\n",
    "        accuracy_curr = accuracy_metric(preds, curr_batch.label)\n",
    "        \n",
    "        loss_curr.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        loss += loss_curr.item()\n",
    "        accuracy += accuracy_curr.item()\n",
    "        \n",
    "    return loss/len(data_iterator), accuracy/len(data_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, data_iterator, loss_func):\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for curr_batch in data_iterator:\n",
    "            sequence, sequence_lengths = curr_batch.text\n",
    "            preds = model(sequence, sequence_lengths)\n",
    "            \n",
    "            loss_curr = loss_func(preds, curr_batch.label)\n",
    "            accuracy_curr = accuracy_metric(preds, curr_batch.label)\n",
    "\n",
    "            loss += loss_curr.item()\n",
    "            accuracy += accuracy_curr.item()\n",
    "        \n",
    "    return loss/len(data_iterator), accuracy/len(data_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "index_select() received an invalid combination of arguments - got (int, NoneType), but expected one of:\n * (int dim, Tensor index)\n      didn't match because some of the arguments have invalid types: (int, !NoneType!)\n * (name dim, Tensor index)\n      didn't match because some of the arguments have invalid types: (!int!, !NoneType!)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m      6\u001b[0m     time_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 8\u001b[0m     training_loss, train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlstm_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     validation_loss, validation_accuracy \u001b[38;5;241m=\u001b[39m validate(lstm_model, valid_data_iterator, loss_func)\n\u001b[0;32m     11\u001b[0m     time_end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[1;32mIn[28], line 9\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, data_iterator, optim, loss_func)\u001b[0m\n\u001b[0;32m      7\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m      8\u001b[0m sequence, sequence_lengths \u001b[38;5;241m=\u001b[39m curr_batch\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m----> 9\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mlstm_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence_lengths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m loss_curr \u001b[38;5;241m=\u001b[39m loss_func(preds, curr_batch\u001b[38;5;241m.\u001b[39mlabel)\n\u001b[0;32m     12\u001b[0m accuracy_curr \u001b[38;5;241m=\u001b[39m accuracy_metric(preds, curr_batch\u001b[38;5;241m.\u001b[39mlabel)\n",
      "File \u001b[1;32mr:\\MasteringPyTorchV2\\Chapter04\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[21], line 23\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, sequence, sequence_lengths)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# embedded_output := (sequence_length, batch_size, embedding_dimension)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m---> 23\u001b[0m     packed_embedded_output \u001b[38;5;241m=\u001b[39m \u001b[43mcuda_pack_padded_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedded_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence_lengths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     packed_embedded_output \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mrnn\u001b[38;5;241m.\u001b[39mpack_padded_sequence(embedded_output, sequence_lengths)\n",
      "Cell \u001b[1;32mIn[20], line 16\u001b[0m, in \u001b[0;36mcuda_pack_padded_sequence\u001b[1;34m(input, lengths, batch_first, enforce_sorted)\u001b[0m\n\u001b[0;32m     14\u001b[0m     sorted_indices \u001b[38;5;241m=\u001b[39m sorted_indices\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     15\u001b[0m batch_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_first \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_select\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msorted_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m data, batch_sizes \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m     19\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_VariableFunctions\u001b[38;5;241m.\u001b[39m_pack_padded_sequence(\u001b[38;5;28minput\u001b[39m, lengths, batch_first)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m PackedSequence(data, batch_sizes, sorted_indices)\n",
      "\u001b[1;31mTypeError\u001b[0m: index_select() received an invalid combination of arguments - got (int, NoneType), but expected one of:\n * (int dim, Tensor index)\n      didn't match because some of the arguments have invalid types: (int, !NoneType!)\n * (name dim, Tensor index)\n      didn't match because some of the arguments have invalid types: (!int!, !NoneType!)\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "best_validation_loss = float('inf')\n",
    "\n",
    "for ep in range(num_epochs):\n",
    "\n",
    "    time_start = time.time()\n",
    "    \n",
    "    training_loss, train_accuracy = train(lstm_model, train_data_iterator, optim, loss_func)\n",
    "    validation_loss, validation_accuracy = validate(lstm_model, valid_data_iterator, loss_func)\n",
    "    \n",
    "    time_end = time.time()\n",
    "    time_delta = time_end - time_start \n",
    "    \n",
    "    if validation_loss < best_validation_loss:\n",
    "        best_validation_loss = validation_loss\n",
    "        torch.save(lstm_model.state_dict(), 'lstm_model.pt')\n",
    "    \n",
    "    print(f'epoch number: {ep+1} | time elapsed: {time_delta}s')\n",
    "    print(f'training loss: {training_loss:.3f} | training accuracy: {train_accuracy*100:.2f}%')\n",
    "    print(f'validation loss: {validation_loss:.3f} |  validation accuracy: {validation_accuracy*100:.2f}%')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.557 | test accuracy: 78.59%\n"
     ]
    }
   ],
   "source": [
    "lstm_model.load_state_dict(torch.load('./lstm_model.pt'))\n",
    "\n",
    "test_loss, test_accuracy = validate(lstm_model, test_data_iterator, loss_func)\n",
    "\n",
    "print(f'test loss: {test_loss:.3f} | test accuracy: {test_accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_inference(model, sentence):\n",
    "    model.eval()\n",
    "    \n",
    "    # text transformations\n",
    "    tokenized = data.get_tokenizer(\"basic_english\")(sentence)\n",
    "    tokenized = [TEXT_FIELD.vocab.stoi[t] for t in tokenized]\n",
    "    \n",
    "    # model inference\n",
    "    model_input = torch.LongTensor(tokenized).to(device)\n",
    "    model_input = model_input.unsqueeze(1)\n",
    "    \n",
    "    pred = torch.sigmoid(model(model_input))\n",
    "    \n",
    "    return pred.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015025223605334759\n",
      "0.006463728845119476\n",
      "0.49227291345596313\n",
      "0.7560229897499084\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_inference(lstm_model, \"This film is horrible\"))\n",
    "print(sentiment_inference(lstm_model, \"Director tried too hard but this film is bad\"))\n",
    "print(sentiment_inference(lstm_model, \"This film will be houseful for weeks\"))\n",
    "print(sentiment_inference(lstm_model, \"I just really loved the movie\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
